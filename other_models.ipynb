{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b846f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 17:40:37.484336: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "from sklearn.svm import SVR\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec12eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 610\n",
    "train_df = pd.read_csv(f'csv/{region}.csv', index_col=0)\n",
    "test_df = pd.read_csv(f'csv2/{region}.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c429966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true,y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y_pred-y_true)))\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    mask = y_true != 0  # Create a mask for non-zero true values\n",
    "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask]))\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(np.divide(numerator, denominator))\n",
    "\n",
    "def getTT(region):\n",
    "    return round((((region-100)//50) * -5.2222 ) + 147.11),round((((region%50)-2) * 5.2083)+20.458 )\n",
    "\n",
    "def mean_absolute_percentage_error_revise(y_true, y_pred):\n",
    "    # RMSE\n",
    "    rmse = K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "    # SMAPE\n",
    "    epsilon = 1e-10  # Small constant to avoid division by zero\n",
    "    smape_numerator = tf.abs(y_true - y_pred)\n",
    "    smape_denominator = tf.maximum(tf.abs(y_true) + tf.abs(y_pred), epsilon)\n",
    "    smape = K.mean(smape_numerator / smape_denominator)\n",
    "\n",
    "    alpha = 0.4 # hyperparameter, adjust alpha as needed\n",
    "    combined_loss = alpha * rmse + (1 - alpha) * smape\n",
    "\n",
    "    return combined_loss\n",
    "def custom_metric(y_true, y_pred):\n",
    "    return mean_absolute_percentage_error_revise(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a251915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df\n",
    "test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0dbaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial + scale CSV\n",
    "\n",
    "# create polynomial features\n",
    "poly = PolynomialFeatures(2,include_bias=False)\n",
    "train_poly = pd.DataFrame(poly.fit_transform(train),columns=poly.get_feature_names_out(train.columns))\n",
    "test_poly = pd.DataFrame(poly.fit_transform(test),columns=poly.get_feature_names_out(test.columns))\n",
    "\n",
    "# initialise scalers\n",
    "minmaxscaler = MinMaxScaler(feature_range=(0.1,1))\n",
    "minmaxscaler.fit(train_poly)\n",
    "minmaxscalerY = MinMaxScaler(feature_range=(0.1,1))\n",
    "minmaxscalerY.fit((train[[str(region)]]))\n",
    "\n",
    "# scale train and test using scalers\n",
    "scaled_train = pd.DataFrame(minmaxscaler.transform(train_poly),columns=train_poly.columns)\n",
    "scaled_test = pd.DataFrame(minmaxscaler.transform(test_poly),columns=test_poly.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36899d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series for CSV\n",
    "## For testing\n",
    "data = scaled_test.values\n",
    "scaled_data = data\n",
    "\n",
    "X2, y2 = [], []\n",
    "sequence_length = 16  # X = 16 previous time steps\n",
    "\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X2.append(scaled_data[i:i+sequence_length, :])  # Use all columns\n",
    "    y2.append(scaled_data[i+sequence_length, 0])  # Assuming prediction is based on the first column\n",
    "\n",
    "X2, y2 = np.array(X2), np.array(y2)\n",
    "\n",
    "## For training\n",
    "data = scaled_train.values\n",
    "scaled_data = data\n",
    "\n",
    "X, y = [], []\n",
    "sequence_length = 16  # X = 16 previous time steps\n",
    "\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i+sequence_length, :])  # Use all columns\n",
    "    y.append(scaled_data[i+sequence_length, 0])  # Assuming prediction is based on the first column\n",
    "\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e9eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.90)  # 90% training, 10% val\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d51cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2664, 16, 324)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606abe48",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f30e525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbab751ae60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7fbab751ae60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 17:41:46.341357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function mean_absolute_percentage_error_revise at 0x7fbac1ea60e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function mean_absolute_percentage_error_revise at 0x7fbac1ea60e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function custom_metric at 0x7fbac1ea6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function custom_metric at 0x7fbac1ea6200> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "84/84 [==============================] - 1s 4ms/step - loss: 0.1183 - custom_metric: 0.1191\n",
      "Epoch 2/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0894 - custom_metric: 0.0893\n",
      "Epoch 3/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0856 - custom_metric: 0.0860\n",
      "Epoch 4/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0870 - custom_metric: 0.0871\n",
      "Epoch 5/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0806 - custom_metric: 0.0807\n",
      "Epoch 6/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0815 - custom_metric: 0.0824\n",
      "Epoch 7/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0758 - custom_metric: 0.0759\n",
      "Epoch 8/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0782 - custom_metric: 0.0781\n",
      "Epoch 9/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0702 - custom_metric: 0.0697\n",
      "Epoch 10/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0729 - custom_metric: 0.0725\n",
      "Epoch 11/25\n",
      "84/84 [==============================] - 0s 3ms/step - loss: 0.0734 - custom_metric: 0.0736\n",
      "Epoch 12/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0697 - custom_metric: 0.0697\n",
      "Epoch 13/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0674 - custom_metric: 0.0681\n",
      "Epoch 14/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0688 - custom_metric: 0.0684\n",
      "Epoch 15/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0708 - custom_metric: 0.0707\n",
      "Epoch 16/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0653 - custom_metric: 0.0650\n",
      "Epoch 17/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0666 - custom_metric: 0.0664\n",
      "Epoch 18/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0644 - custom_metric: 0.0643\n",
      "Epoch 19/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0645 - custom_metric: 0.0646\n",
      "Epoch 20/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0615 - custom_metric: 0.0617\n",
      "Epoch 21/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0653 - custom_metric: 0.0655\n",
      "Epoch 22/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0621 - custom_metric: 0.0619\n",
      "Epoch 23/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0592 - custom_metric: 0.0591\n",
      "Epoch 24/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0629 - custom_metric: 0.0630\n",
      "Epoch 25/25\n",
      "84/84 [==============================] - 0s 2ms/step - loss: 0.0603 - custom_metric: 0.0610\n"
     ]
    }
   ],
   "source": [
    "X = X_train\n",
    "Y = y_train\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(16, 324), name='input_layer'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid', name='output_layer')\n",
    "])\n",
    "model.compile(optimizer='adam', loss=mean_absolute_percentage_error_revise, metrics=[custom_metric])\n",
    "\n",
    "history = model.fit(X, Y, epochs=25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f64c9589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbac1db2dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x7fbac1db2dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "93/93 [==============================] - 0s 2ms/step\n",
      "0.07682666149066743\n",
      "0.1756792419036884\n",
      "0.1861012093766768\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X2)\n",
    "print(rmse(y2,predictions))\n",
    "print(mape(y2,predictions))\n",
    "print(smape(y2,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97670ad6",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45409ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flattened = X_train.reshape((X_train.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500e235c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=3, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=5, n_jobs=0,\n",
       "             num_parallel_tree=1, objective='reg:logistic', predictor='auto',\n",
       "             random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_r = xg.XGBRegressor(objective ='reg:logistic', \n",
    "                  n_estimators = 5, max_depth=3) \n",
    "xgb_r.fit(X_flattened, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "697e573c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2960, 16, 324)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f79c8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09198155100162343\n",
      "0.7719815176725909\n",
      "0.5435671859569068\n"
     ]
    }
   ],
   "source": [
    "predictions = xgb_r.predict(X2.reshape((X2.shape[0], -1)))\n",
    "print(rmse(y2,predictions))\n",
    "print(mape(y2,predictions))\n",
    "print(smape(y2,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d82a407",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce9aa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flattened = X_train.reshape((X_train.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e136f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=20, degree=2, epsilon=0.01, gamma=0.1, kernel='poly')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_model = SVR(kernel='poly', degree=2, gamma=0.1,C=20,epsilon=0.01) \n",
    "svr_model.fit(X_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6eb47dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08760058657658516\n",
      "0.390232696910985\n",
      "0.3671115321489117\n"
     ]
    }
   ],
   "source": [
    "predictions = svr_model.predict(X2.reshape((X2.shape[0], -1)))\n",
    "print(rmse(y2,predictions))\n",
    "print(mape(y2,predictions))\n",
    "print(smape(y2,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846010c7",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c784373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_train_df = train_df[str(region)]\n",
    "arima_test_df = test_df[str(region)]\n",
    "arima_train_df_reshaped = arima_train_df.values.reshape(-1, 1)\n",
    "arima_test_df_reshaped = arima_test_df.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2582ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise scalers\n",
    "scaler = MinMaxScaler(feature_range=(0.1,1))\n",
    "scaler.fit(arima_train_df_reshaped)\n",
    "\n",
    "arima_scaled_train = scaler.transform(arima_train_df_reshaped)\n",
    "arima_scaled_test = scaler.transform(arima_test_df_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ad7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "d = 1\n",
    "q = 1\n",
    "\n",
    "model = ARIMA(arima_scaled_train, order=(p, d, q))\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ffd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0671422379152212\n",
      "0.2608799746141603\n",
      "0.26342642700657853\n"
     ]
    }
   ],
   "source": [
    "predictions = model_fit.predict(start=len(arima_scaled_train), end=len(arima_scaled_train) + len(arima_scaled_test) - 1, typ='levels')\n",
    "print(rmse(arima_scaled_test.flatten(),predictions))\n",
    "print(mape(arima_scaled_test.flatten(),predictions))\n",
    "print(smape(arima_scaled_test.flatten(),predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d18b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e281d6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
